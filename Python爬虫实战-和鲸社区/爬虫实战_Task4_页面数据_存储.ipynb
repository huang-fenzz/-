{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175a6cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from lxml import etree\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a3dbb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\n",
    "    'User-Agent' :\"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:93.0) Gecko/20100101 Firefox/93.0\"\n",
    "}\n",
    "r = requests.get('https://book.douban.com/subject/1200840/comments/',headers=headers)\n",
    "print(r.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe89d867",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对于昨日作业的巩固 直接在谷歌中复制只有 //*[@id=\"comments\"]/div[1]/ul/li[1]/div[2]/p ,通过观察响应数据自己添加了/span即可\n",
    "\n",
    "#这个功能只能读取一条评论\n",
    "html  = etree.HTML(r.text)\n",
    "result = html.xpath('//*[@id=\"comments\"]/div[1]/ul/li[1]/div[2]/p/span/text()')\n",
    "print(''.join(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c22dd30d",
   "metadata": {},
   "source": [
    "## 在网页中发现,li对应着不同的评论数据,尝试将所有评论爬去下来!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de96cff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "html  = etree.HTML(r.text)\n",
    "#此时将li的[]去掉了,对比上个代码框\n",
    "result = html.xpath('//*[@id=\"comments\"]/div[1]/ul/li/div[2]/p/span/text()')   \n",
    "#已经成功将所有评论爬去下来,但成为了一个集合,不利于后续处理;\n",
    "print(''.join(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63ad925",
   "metadata": {},
   "source": [
    "## 尝试将爬去下来的数据逐个添加到list列表中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb763ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_list=[]\n",
    "html  = etree.HTML(r.text)\n",
    "#此时去掉了/text()\n",
    "lis = html.xpath('//*[@id=\"comments\"]/div[1]/ul/li/div[2]/p/span')  \n",
    "#分批次提取评论并将结果转化为str存入列表中\n",
    "for li in lis:\n",
    "    result = li.xpath('./text()')\n",
    "    comments_list.append(''.join(result))\n",
    "print(comments_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c2474e9",
   "metadata": {},
   "source": [
    "## 爬去完毕且清洗干净后即可保存数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ed988f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#txt文件保存方式\n",
    "with open(r'G:\\Git_\\网络爬虫\\Python爬虫实战-和鲸社区\\输出\\Task4.txt','w',encoding='utf-8') as f:\n",
    "    for i in comments_list:\n",
    "        f.write(i+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4aae21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#csv文件保存方式\n",
    "import csv\n",
    "headers = ['No','name','age']\n",
    "values = [\n",
    "    ['01','Blue',18],\n",
    "    ['02','Lble',19],\n",
    "    ['03','Uble',20],\n",
    "]\n",
    "with open(r'G:\\Git_\\网络爬虫\\Python爬虫实战-和鲸社区\\输出\\Task4.csv','w',newline='') as fp:\n",
    "    #获取写入对象\n",
    "    writer = csv.writer(fp)\n",
    "    #写入标题\n",
    "    writer.writerow(headers)\n",
    "    #写入内容\n",
    "    writer.writerows(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10765db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#读取csv文件\n",
    "\n",
    "#方法1 - list格式\n",
    "with open(r'G:\\Git_\\网络爬虫\\Python爬虫实战-和鲸社区\\输出\\Task4.csv','r',encoding='utf-8') as csvfile:\n",
    "    #获取读取对象\n",
    "    reader = csv.reader(csvfile)\n",
    "    #按行输出\n",
    "    for row in reader:\n",
    "        print(row)\n",
    "        \n",
    "#方法2 - dataframe格式\n",
    "a = pd.read_csv(r'G:\\Git_\\网络爬虫\\Python爬虫实战-和鲸社区\\输出\\Task4.csv')\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d1afa5",
   "metadata": {},
   "source": [
    "# 上述技巧合并使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b884bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://book.douban.com/subject/1200840/comments/'\n",
    "headers = {\n",
    "    'User-Agent' :\"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:93.0) Gecko/20100101 Firefox/93.0\"\n",
    "}\n",
    "comments_list = []\n",
    "response = requests.get(url,headers=headers)\n",
    "html = etree.HTML(response.text)\n",
    "lis = html.xpath('//*[@id=\"comments\"]/div[1]/ul/li/div[2]/p/span') \n",
    "for li in lis:\n",
    "    result = li.xpath('./text()')\n",
    "    comments_list.append(''.join(result))\n",
    "    \n",
    "#comments_list有多少个内容就创建多大的列表\n",
    "new_list = [[x] for x in comments_list]\n",
    "with open(r'G:\\Git_\\网络爬虫\\Python爬虫实战-和鲸社区\\输出\\Task4_1.csv',mode='w',newline='',encoding='utf-8') as f:\n",
    "    csv_file = csv.writer(f)\n",
    "    for i in new_list:\n",
    "        csv_file.writerow(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a967c4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pandas保存方法\n",
    "\n",
    "# new_list = pd.DataFrame(comments_list)\n",
    "# new_list.to_excel(r'G:\\Git_\\网络爬虫\\Python爬虫实战-和鲸社区\\输出\\Task4_实战测试.xlsx')\n",
    "# new_list.to_csv(r'G:\\Git_\\网络爬虫\\Python爬虫实战-和鲸社区\\输出\\Task4_实战测试.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4464accb",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'G:\\Git_\\网络爬虫\\Python爬虫实战-和鲸社区\\输出\\Task4_1.csv',mode='r',newline='',encoding='utf-8') as fp:\n",
    "    reader = csv.reader(fp)\n",
    "    for i in reader:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958b8e65",
   "metadata": {},
   "source": [
    "# 媒帮热搜榜实战"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3471da",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\n",
    "    'Connection': 'keep-alive',\n",
    "    'Cache-Control': 'max-age=0',\n",
    "    'Upgrade-Insecure-Requests': '1',\n",
    "    'User-Agent' :\"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:93.0) Gecko/20100101 Firefox/93.0\",\n",
    "    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9',\n",
    "    'Referer': 'http://hot.meibp.com/',\n",
    "    'Accept-Language': 'zh-CN,zh;q=0.9',\n",
    "}\n",
    "response = requests.get('http://hot.meibp.com/',headers=headers,verify=False)\n",
    "html = etree.HTML(response.text)\n",
    "\n",
    "#链接和标题的xpath\n",
    "titles = html.xpath('//*[@id=\"hot\"]/div/div/div/div/div[1]/a/@title')\n",
    "links = html.xpath('//*[@id=\"hot\"]/div/div/div/div/div[1]/a/@href')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d92bcf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for i in range(len(titles)):\n",
    "    result = {\n",
    "        '标题':titles[i],\n",
    "        '链接':links[i]\n",
    "    }\n",
    "    print(result)\n",
    "    results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb652772",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'G:\\Git_\\网络爬虫\\Python爬虫实战-和鲸社区\\输出\\Task4.json',mode='w') as file:\n",
    "    file.write(json.dumps(results,indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f8542a",
   "metadata": {},
   "source": [
    "# 作业"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72cf573b",
   "metadata": {},
   "source": [
    "目标网站： http://123.meibp.com/neirongyunying\n",
    "\n",
    "目标：通过以下获取html提取内容，保存方式建议excel、csv既可"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e475f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\n",
    "    'Connection': 'keep-alive',\n",
    "    'Cache-Control': 'max-age=0',\n",
    "    'Upgrade-Insecure-Requests': '1',\n",
    "    'User-Agent' :\"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:93.0) Gecko/20100101 Firefox/93.0\",\n",
    "    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9',\n",
    "    'Referer': 'http://hot.meibp.com/',\n",
    "    'Accept-Language': 'zh-CN,zh;q=0.9',\n",
    "}\n",
    "response = requests.get('http://123.meibp.com/neirongyunying/',headers=headers,verify=False)\n",
    "html = etree.HTML(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f15d685",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "titles = html.xpath('//*[@id=\"64\"]/div/div/div/div/a[1]/h3/text()')\n",
    "words = html.xpath('//*[@id=\"64\"]/div/div/div/div/a[2]/p/text()')\n",
    "results =[]\n",
    "for i in range(len(words)):\n",
    "    i = {\n",
    "        '标题':titles[i],\n",
    "        '内容':words[i]\n",
    "       }\n",
    "    print(i)\n",
    "    results.append(i)\n",
    "new_result = pd.DataFrame(results)\n",
    "new_result.to_excel(r'G:\\Git_\\网络爬虫\\Python爬虫实战-和鲸社区\\输出\\Task4_作业.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9afd0d91",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
